"""
–ü–∞–π–ø–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–¥–∞–∂.

–û—Å–Ω–æ–≤–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏:
- –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
- –û–±—É—á–µ–Ω–∏–µ baseline –º–æ–¥–µ–ª–∏ (ARIMA)
- –û–±—É—á–µ–Ω–∏–µ ML –º–æ–¥–µ–ª–µ–π (LightGBM, XGBoost, RandomForest)
- –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
"""

import pandas as pd
import numpy as np
import joblib
import json
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
import warnings

# ML –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import TimeSeriesSplit
import lightgbm as lgb
import xgboost as xgb

# –ú–µ—Ç—Ä–∏–∫–∏
from .metrics import smape, calculate_regression_metrics

warnings.filterwarnings('ignore')


# ============================================================================
# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
# ============================================================================

def prepare_data_for_training(
    train_df: pd.DataFrame,
    val_df: Optional[pd.DataFrame] = None,
    test_df: Optional[pd.DataFrame] = None,
    target_col: str = 'sales',
    exclude_cols: Optional[List[str]] = None
) -> Tuple[pd.DataFrame, pd.Series, Optional[pd.DataFrame], Optional[pd.Series], 
           Optional[pd.DataFrame], Optional[pd.Series], List[str]]:
    """
    –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π.
    
    –í–ê–ñ–ù–û: –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç data leakage - –ø—Ä–æ–ø—É—Å–∫–∏ –∑–∞–ø–æ–ª–Ω—è—é—Ç—Å—è —Ç–æ–ª—å–∫–æ –Ω–∞ train,
    –∑–∞—Ç–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è –∫ val/test.
    
    Args:
        train_df: –û–±—É—á–∞—é—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç —Å —Ñ–∏—á–∞–º–∏
        val_df: –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        test_df: –¢–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        target_col: –ù–∞–∑–≤–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –∫–æ–ª–æ–Ω–∫–∏
        exclude_cols: –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è –∏–∑ —Ñ–∏—á–µ–π
        
    Returns:
        Tuple —Å (X_train, y_train, X_val, y_val, X_test, y_test, feature_names)
    """
    if exclude_cols is None:
        exclude_cols = ['date', 'store', 'item', target_col]
    
    # –í—ã–±–∏—Ä–∞–µ–º —Ñ–∏—á–∏ (–∏—Å–∫–ª—é—á–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏)
    feature_cols = [col for col in train_df.columns if col not in exclude_cols]
    
    # Train
    X_train = train_df[feature_cols].copy()
    y_train = train_df[target_col].copy()
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤ –Ω–∞ train (–≤—ã—á–∏—Å–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏)
    # –ó–∞–ø–æ–ª–Ω—è–µ–º –º–µ–¥–∏–∞–Ω–æ–π –¥–ª—è —á–∏—Å–ª–æ–≤—ã—Ö —Ñ–∏—á–µ–π
    numeric_cols = X_train.select_dtypes(include=[np.number]).columns
    fill_values = X_train[numeric_cols].median()
    
    X_train[numeric_cols] = X_train[numeric_cols].fillna(fill_values)
    X_train = X_train.fillna(0)  # –î–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
    
    # Validation
    X_val = None
    y_val = None
    if val_df is not None:
        X_val = val_df[feature_cols].copy()
        y_val = val_df[target_col].copy()
        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏–∑ train
        X_val[numeric_cols] = X_val[numeric_cols].fillna(fill_values)
        X_val = X_val.fillna(0)
    
    # Test
    X_test = None
    y_test = None
    if test_df is not None:
        X_test = test_df[feature_cols].copy()
        if target_col in test_df.columns:
            y_test = test_df[target_col].copy()
        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏–∑ train
        X_test[numeric_cols] = X_test[numeric_cols].fillna(fill_values)
        X_test = X_test.fillna(0)
    
    return X_train, y_train, X_val, y_val, X_test, y_test, feature_cols


# ============================================================================
# Baseline: ARIMA
# ============================================================================

class ARIMABaseline:
    """
    –ü—Ä–æ—Å—Ç–∞—è ARIMA baseline –º–æ–¥–µ–ª—å –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤.
    
    –î–ª—è –∫–∞–∂–¥–æ–≥–æ (store, item) –∫–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω—ã–π –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Ä—è–¥,
    –Ω–∞ –∫–æ—Ç–æ—Ä–æ–º –æ–±—É—á–∞–µ—Ç—Å—è ARIMA.
    """
    
    def __init__(self, order: Tuple[int, int, int] = (1, 1, 1), random_state: int = 42):
        """
        Args:
            order: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã ARIMA (p, d, q)
            random_state: –î–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
        """
        self.order = order
        self.random_state = random_state
        self.models = {}  # –°–ª–æ–≤–∞—Ä—å –º–æ–¥–µ–ª–µ–π –¥–ª—è –∫–∞–∂–¥–æ–π (store, item) –ø–∞—Ä—ã
        self.is_fitted = False
        
    def fit(self, df: pd.DataFrame, target_col: str = 'sales'):
        """
        –û–±—É—á–µ–Ω–∏–µ ARIMA –º–æ–¥–µ–ª–µ–π –¥–ª—è –∫–∞–∂–¥–æ–π (store, item) –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏.
        
        Args:
            df: DataFrame —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ date, store, item, target_col
            target_col: –ù–∞–∑–≤–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –∫–æ–ª–æ–Ω–∫–∏
        """
        try:
            from statsmodels.tsa.arima.model import ARIMA
        except ImportError:
            raise ImportError("statsmodels –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install statsmodels")
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –¥–∞—Ç–µ
        df = df.sort_values(['store', 'item', 'date']).copy()
        
        # –û–±—É—á–µ–Ω–∏–µ –æ—Ç–¥–µ–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–∞–∂–¥–æ–π (store, item) –ø–∞—Ä—ã
        for (store, item), group in df.groupby(['store', 'item']):
            ts = group.set_index('date')[target_col].sort_index()
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Ä—è–¥—ã
            if len(ts) < max(self.order) + 2:
                continue
            
            try:
                model = ARIMA(ts, order=self.order)
                fitted_model = model.fit()
                self.models[(store, item)] = fitted_model
            except Exception as e:
                # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –æ–±—É—á–∏—Ç—å ARIMA, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                warnings.warn(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—É—á–∏—Ç—å ARIMA –¥–ª—è (store={store}, item={item}): {e}")
                continue
        
        self.is_fitted = True
        return self
    
    def predict(self, df: pd.DataFrame, steps: int = 1) -> np.ndarray:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
        
        Args:
            df: DataFrame —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ date, store, item
            steps: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –≤–ø–µ—Ä–µ–¥ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            
        Returns:
            –ú–∞—Å—Å–∏–≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        """
        if not self.is_fitted:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞! –í—ã–∑–æ–≤–∏—Ç–µ fit() —Å–Ω–∞—á–∞–ª–∞.")
        
        df = df.sort_values(['store', 'item', 'date']).copy()
        predictions = []
        
        for (store, item), group in df.groupby(['store', 'item']):
            if (store, item) in self.models:
                try:
                    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ —Å–ª–µ–¥—É—é—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
                    forecast = self.models[(store, item)].forecast(steps=steps)
                    predictions.extend(forecast.values if hasattr(forecast, 'values') else [forecast])
                except Exception:
                    # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
                    # –∏–ª–∏ —Å—Ä–µ–¥–Ω–µ–µ –ø–æ –≥—Ä—É–ø–ø–µ
                    last_value = group.get('sales', pd.Series([0])).iloc[-1] if 'sales' in group.columns else 0
                    predictions.extend([last_value] * len(group))
            else:
                # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
                predictions.extend([0] * len(group))
        
        # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
        if len(predictions) < len(df):
            predictions.extend([0] * (len(df) - len(predictions)))
        
        return np.array(predictions[:len(df)])


# ============================================================================
# –û–±—É—á–µ–Ω–∏–µ ML –º–æ–¥–µ–ª–µ–π
# ============================================================================

def train_model(
    model: Any,
    X_train: pd.DataFrame,
    y_train: pd.Series,
    X_val: Optional[pd.DataFrame] = None,
    y_val: Optional[pd.Series] = None,
    random_state: int = 42
) -> Tuple[Any, Dict[str, float], np.ndarray, Optional[np.ndarray]]:
    """
    –û–±—É—á–µ–Ω–∏–µ –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫.
    
    Args:
        model: –ú–æ–¥–µ–ª—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (–¥–æ–ª–∂–Ω–∞ –∏–º–µ—Ç—å fit() –∏ predict())
        X_train: –û–±—É—á–∞—é—â–∏–µ —Ñ–∏—á–∏
        y_train: –û–±—É—á–∞—é—â–∞—è —Ü–µ–ª—å
        X_val: –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ñ–∏—á–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        y_val: –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è —Ü–µ–ª—å (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        random_state: –î–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
        
    Returns:
        Tuple —Å (–æ–±—É—á–µ–Ω–Ω–∞—è_–º–æ–¥–µ–ª—å, –º–µ—Ç—Ä–∏–∫–∏, y_pred_train, y_pred_val)
    """
    # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ random_state –µ—Å–ª–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
    if hasattr(model, 'random_state'):
        model.random_state = random_state
    if hasattr(model, 'seed'):
        model.seed = random_state
    
    # –û–±—É—á–µ–Ω–∏–µ
    model.fit(X_train, y_train)
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    y_pred_train = model.predict(X_train)
    
    # –ú–µ—Ç—Ä–∏–∫–∏ –Ω–∞ train
    train_metrics = calculate_regression_metrics(y_train.values, y_pred_train)
    train_metrics = {f'train_{k}': v for k, v in train_metrics.items()}
    
    # –ú–µ—Ç—Ä–∏–∫–∏ –Ω–∞ validation
    val_metrics = {}
    y_pred_val = None
    if X_val is not None and y_val is not None:
        y_pred_val = model.predict(X_val)
        val_metrics = calculate_regression_metrics(y_val.values, y_pred_val)
        val_metrics = {f'val_{k}': v for k, v in val_metrics.items()}
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
    all_metrics = {**train_metrics, **val_metrics}
    
    return model, all_metrics, y_pred_train, y_pred_val


def create_default_models(random_state: int = 42) -> Dict[str, Any]:
    """
    –°–æ–∑–¥–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å –º–æ–¥–µ–ª–µ–π –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é.
    
    Args:
        random_state: –î–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
        
    Returns:
        –°–ª–æ–≤–∞—Ä—å {–Ω–∞–∑–≤–∞–Ω–∏–µ: –º–æ–¥–µ–ª—å}
    """
    models = {
        'LightGBM': lgb.LGBMRegressor(
            n_estimators=100,
            learning_rate=0.1,
            max_depth=7,
            random_state=random_state,
            verbose=-1,
            n_jobs=-1
        ),
        'XGBoost': xgb.XGBRegressor(
            n_estimators=100,
            learning_rate=0.1,
            max_depth=7,
            random_state=random_state,
            n_jobs=-1
        ),
        'RandomForest': RandomForestRegressor(
            n_estimators=100,
            max_depth=10,
            random_state=random_state,
            n_jobs=-1
        )
    }
    
    return models


def train_models_with_cv(
    models_dict: Dict[str, Any],
    X_train: pd.DataFrame,
    y_train: pd.Series,
    cv_strategy: Optional[TimeSeriesSplit] = None,
    random_state: int = 42
) -> Dict[str, Dict]:
    """
    –û–±—É—á–µ–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π —Å –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–µ–π.
    
    Args:
        models_dict: –°–ª–æ–≤–∞—Ä—å {–Ω–∞–∑–≤–∞–Ω–∏–µ: –º–æ–¥–µ–ª—å}
        X_train: –û–±—É—á–∞—é—â–∏–µ —Ñ–∏—á–∏
        y_train: –û–±—É—á–∞—é—â–∞—è —Ü–µ–ª—å
        cv_strategy: –°—Ç—Ä–∞—Ç–µ–≥–∏—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é TimeSeriesSplit)
        random_state: –î–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
        
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
    """
    if cv_strategy is None:
        cv_strategy = TimeSeriesSplit(n_splits=3)
    
    results = {}
    
    for model_name, model in models_dict.items():
        print(f"\nüîÑ –û–±—É—á–µ–Ω–∏–µ {model_name}...")
        
        cv_scores = {'SMAPE': [], 'RMSE': [], 'MAE': [], 'R¬≤': []}
        
        for fold_idx, (train_idx, val_idx) in enumerate(cv_strategy.split(X_train), 1):
            # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ fold train –∏ fold val
            X_fold_train = X_train.iloc[train_idx]
            y_fold_train = y_train.iloc[train_idx]
            X_fold_val = X_train.iloc[val_idx]
            y_fold_val = y_train.iloc[val_idx]
            
            # –û–±—É—á–µ–Ω–∏–µ
            model_copy = type(model)(**model.get_params()) if hasattr(model, 'get_params') else model
            model_copy.fit(X_fold_train, y_fold_train)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            y_pred = model_copy.predict(X_fold_val)
            
            # –ú–µ—Ç—Ä–∏–∫–∏
            metrics = calculate_regression_metrics(y_fold_val.values, y_pred)
            for metric_name, value in metrics.items():
                cv_scores[metric_name].append(value)
        
        # –°—Ä–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –ø–æ —Ñ–æ–ª–¥–∞–º
        avg_metrics = {metric: np.mean(scores) for metric, scores in cv_scores.items()}
        std_metrics = {f'{metric}_std': np.std(scores) for metric, scores in cv_scores.items()}
        
        results[model_name] = {
            'metrics': {**avg_metrics, **std_metrics},
            'cv_scores': cv_scores
        }
        
        print(f"   SMAPE: {avg_metrics['SMAPE']:.4f} ¬± {std_metrics['SMAPE_std']:.4f}")
    
    return results


# ============================================================================
# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∏ –∞–Ω–∞–ª–∏–∑
# ============================================================================

def compare_models(results_dict: Dict[str, Dict]) -> pd.DataFrame:
    """
    –°–æ–∑–¥–∞–µ—Ç —Å–≤–æ–¥–Ω—É—é —Ç–∞–±–ª–∏—Ü—É –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π.
    
    Args:
        results_dict: –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –º–æ–¥–µ–ª–µ–π
        
    Returns:
        DataFrame —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
    """
    comparison_data = []
    
    for model_name, result in results_dict.items():
        metrics = result.get('metrics', {})
        row = {'Model': model_name}
        row.update(metrics)
        comparison_data.append(row)
    
    df = pd.DataFrame(comparison_data)
    
    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ SMAPE (–µ—Å–ª–∏ –µ—Å—Ç—å)
    if 'SMAPE' in df.columns:
        df = df.sort_values('SMAPE')
    
    return df


def get_feature_importance(model: Any, feature_names: List[str], top_n: int = 20) -> pd.DataFrame:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å —Ñ–∏—á–µ–π –∏–∑ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏.
    
    Args:
        model: –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
        feature_names: –°–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π —Ñ–∏—á–µ–π
        top_n: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø —Ñ–∏—á–µ–π
        
    Returns:
        DataFrame —Å –≤–∞–∂–Ω–æ—Å—Ç—å—é —Ñ–∏—á–µ–π
    """
    importance_dict = {}
    
    # –†–∞–∑–Ω—ã–µ —Å–ø–æ—Å–æ–±—ã –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
    if hasattr(model, 'feature_importances_'):
        importance_values = model.feature_importances_
    elif hasattr(model, 'get_feature_importance'):
        importance_values = model.get_feature_importance()
    else:
        return pd.DataFrame({'feature': feature_names, 'importance': [0] * len(feature_names)})
    
    # –°–æ–∑–¥–∞–µ–º DataFrame
    importance_df = pd.DataFrame({
        'feature': feature_names,
        'importance': importance_values
    }).sort_values('importance', ascending=False).head(top_n)
    
    return importance_df


# ============================================================================
# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
# ============================================================================

def save_model(
    model: Any,
    filepath: Path,
    metrics: Optional[Dict[str, float]] = None,
    feature_names: Optional[List[str]] = None
):
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª—å –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ.
    
    Args:
        model: –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
        filepath: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è (.joblib —Ñ–∞–π–ª)
        metrics: –ú–µ—Ç—Ä–∏–∫–∏ –º–æ–¥–µ–ª–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        feature_names: –°–ø–∏—Å–æ–∫ —Ñ–∏—á–µ–π (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    """
    filepath = Path(filepath)
    filepath.parent.mkdir(parents=True, exist_ok=True)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    joblib.dump(model, filepath)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
    metadata = {}
    if metrics is not None:
        metadata['metrics'] = metrics
    if feature_names is not None:
        metadata['feature_names'] = feature_names
    
    if metadata:
        metadata_path = filepath.with_suffix('.json')
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
    
    print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {filepath}")
    if metadata:
        print(f"   –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {metadata_path}")


def load_model(filepath: Path) -> Tuple[Any, Optional[Dict]]:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ.
    
    Args:
        filepath: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –º–æ–¥–µ–ª–∏
        
    Returns:
        Tuple —Å (–º–æ–¥–µ–ª—å, –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ)
    """
    filepath = Path(filepath)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
    model = joblib.load(filepath)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö (–µ—Å–ª–∏ –µ—Å—Ç—å)
    metadata_path = filepath.with_suffix('.json')
    metadata = None
    if metadata_path.exists():
        with open(metadata_path, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
    
    return model, metadata

